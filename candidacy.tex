\documentclass[xcolor={table}]{beamer}
\usepackage{tabularx}
\usepackage{ulem}
\usepackage{color,soul}
\usepackage[table]{xcolor}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{graphicx,import}
\usepackage{tcolorbox}

\makeatletter
\newcommand\SoulColor{%
    \let\set@color\beamerorig@set@color
    \let\reset@color\beamerorig@reset@color}
\makeatother

\usetheme{metropolis}           % Use metropolis theme
\title{Multi-Document Summarization: Evaluation, Extraction, and Abstraction}
\date{\today}
\author{Chris Kedzie}
\institute{Dept. of Computer Science, Columbia University}
\begin{document}
  \maketitle


%\begin{frame}{Presentation Overview}
%
%\begin{enumerate}
%\only<1>{\item Task Definition \& Variations}
%\only<2>{\item \textbf{Task Definition \& Variations}}
%\item Evaluation
%\item Two Implementation Paradigms
%\begin{enumerate}
%\item Extractive Summarization
%\item Abstractive Summarization
%\end{enumerate}
%\end{enumerate}
%
%\end{frame}
%
%
%\section{Summarization Task}
%
%
%
%
%\begin{frame}{Summary Tasks}
%\only<1>{\resizebox{\textwidth}{!}{\Huge{\import{svg/}{tasks1.pdf_tex}}}}
%\only<2>{\resizebox{\textwidth}{!}{\Huge{\import{svg/}{tasks2.pdf_tex}}}}
%\only<3>{\resizebox{\textwidth}{!}{\Huge{\import{svg/}{tasks3.pdf_tex}}}}
%\only<4>{\resizebox{\textwidth}{!}{\Huge{\import{svg/}{tasks4.pdf_tex}}}}
%\only<5>{\resizebox{\textwidth}{!}{\Huge{\import{svg/}{tasks5.pdf_tex}}}}
%\end{frame}
%
%
%
%
%
%\section{Evaluation}
%
%\begin{frame}{\textbf{Intrinsic} vs \textbf{Extrinsic} Evaluation}
%
%\textbf{Intrinsic Evaluation}
%\begin{itemize}
%\item Are the summaries grammatical, coherent, and otherwise well written?
%\only<2->{\textbf{(Linguistic Quality)}}
%\item Do they convey the important information in the input documents?
%    \only<3->{\textbf{(Coverage \& Responsiveness)}}
%\end{itemize}
%\textbf{Extrinsic Evaluation}
%\begin{itemize}
%\item Do the summaries help in some downstream task? 
%\end{itemize}
%\end{frame}
%
%
%\begin{frame}{Summary Content Evaluation}
%\textbf{Coverage Assessment} 
%\begin{itemize}
%    \item How much of the meaning in 
%        a reference summary is covered by the system summary? (6 pt. scale)
%\end{itemize}
%~\\
%~\\
%\textbf{Responsiveness Assessment} 
%\begin{itemize}
%    \item  How well does a summary answer the information need,
%        given the topic query/user profile? (5 pt. scale)
%\end{itemize}
%\end{frame}
%
%\begin{frame}{Content Evaluation Road Map}
% \begin{tikzpicture}[
%    dep/.style={draw, shape=circle, ultra thick, fill=blue!20},
%    depa/.style={draw, shape=circle, ultra thick, fill=green},
%]
%
%\draw[fill=none,draw=none] (-2,1.6) rectangle (8,-5.3);
%\visible<2->{
%\draw[fill=blue!5,draw=none] (-2,1.6) rectangle (8,-1.8);
%\draw[fill=red!5,draw=none] (-2,-5.3) rectangle (8,-1.8);
%\draw[dashed] (-2,-1.8) -- (8,-1.8);
%\node (a) at (0,0) {\textbf{Shallow Coverage}};
%\node[align=center] (b) at (6.2,-3.0) 
%    {\textbf{Semantically}\\ \textbf{Informed Coverage}};
%}
%
%    \visible<3->{\draw[thick,dotted] (4.9,0.6) rectangle (7.5,-1.1);
%\draw[thick,dotted] (-1.6,-1.9) rectangle (1.6,-5.3);
%\draw[thick,dotted] (1.7,1.6) rectangle (4.3,-4.3);
%    \node[align=center] (x) at (0,-5) {\small \textbf{Needs Reference}\\
%        \small \textbf{Human Annotation}\\};
%    \node[align=center] (y) at (3,1) {\small \textbf{Reference Only}\\};
%    \node[align=center] (z) at (6.2,0) {\small \textbf{Reference Free}\\};
%}
%
%    \node[dep] (linnode) at (3,.2) {};
%    \node[] (lintxt) at (3,-.2) {\tiny \cite{lin2004rouge}};
%    \node[dep] (conroynode) at (3,-1) {};
%    \node[] (conroytxt) at (3,-1.4) {\tiny \cite{conroy2011nouveau}};
%
%    %\path [->] (A) edge (B); 
%
%    \node[dep] (teufelnode) at (0,-2.2) {};
%    \node[] (teufeltxt) at (0,-2.6) {\tiny \cite{teufel2004evaluating}};
%    \node[dep] (nenkovanode) at (0,-3.2) {};
%    \node[] (nenkovatxt) at (0,-3.6) {\tiny \cite{nenkova2007pyramid}};
%
%    \visible<4->{
%    \node[depa] (teufelnode) at (0,-2.2) {};
%    \node[depa] (nenkovanode) at (0,-3.2) {};
%\draw[thick,draw=green] (-1.6,-1.9) rectangle (1.6,-5.3);
%\node[] (teufeltxt) at (0,-2.6) {\tiny\textcolor{green}{ \cite{teufel2004evaluating}}};
%\node[] (nenkovatxt) at (0,-3.6) {\tiny\textcolor{green}{\cite{nenkova2007pyramid}}};
%
%\node[align=center] (x) at (0,-5) {\small \textcolor{green}{\textbf{Needs Reference}}\\
%\small \textcolor{green}{\textbf{Human Annotation}}\\};
%    }
%
%    \node[dep] (hennignode) at (3,-2.7) {};
%    \node[] (hennigtxt) at (3,-3.1) {\tiny \cite{hennig2010learning}};
%    \node[dep] (yangnode) at (3,-3.7) {};
%    \node[] (yangtxt) at (3,-4.1) {\tiny \cite{yang2016peak}};
%
%    \node[dep] (louisnode) at (6.2,-.5) {};
%    \node[] (louistxt) at (6.2, -.9) {\tiny \cite{louis2009automatically}};
%    \path [->] (nenkovanode) edge (hennignode); 
%    \path [->] (nenkovanode) edge (yangnode); 
%
%\end{tikzpicture}
%
%\end{frame}
%
%\begin{frame}[t]{Factoids and Pyramids}
%    \textbf{Factoids} \cite{teufel2004evaluating} \\
%    \textbf{Pyramid} \cite{nenkova2007pyramid}
%    \begin{itemize}
%        \item \small{Label factoids/SCUs in ref. summaries}
%        \uncover<5->{\item \small{Weight factoids/SCUs by frequency in 
%            references }}
%        \uncover<7->{\item \small{Summary score $\propto$ to the sum of 
%            weights of factoids/SCUs contained}}
%    \end{itemize}
%
%    ~\\
%    
%    
%    \uncover<2->{\small SCU label: \textit{Lopez left GM for VW}\\} 
%    \uncover<6->{\small SCU Weight: 2\\}
%
%    \only<3->{
%    \begin{itemize}
%        \only<-3>{   \tiny{ \item
%    The industrial espionage case involving GM and VW began with 
%    the hiring of Jose Ignacio Lopez, an employee of GM
%    subsidiary Adam Opel, by VW as a production director.}
%}\only<4->{\tiny{ \item
%    The industrial espionage case involving GM and VW began with 
%    \SoulColor\hl{the} \SoulColor\hl{hiring of Jose Ignacio Lopez, an employee of GM} 
%    subsidiary Adam Opel, \SoulColor\hl{by VW} as a production director.}
%    
%}
% 
%    \only<-3>{
%    \item \tiny{
%    However, he left GM for VW under circumstances, which along
%    with ensuing events, were described by a German judge as ``potentially the
%    biggest ever case of industrial espionage.''}}
%    \only<4->{
%    \item \tiny{
%    However, \SoulColor\hl{he left GM for VW} under circumstances, which along
%    with ensuing events, were described by a German judge as ``potentially the
%    biggest ever case of industrial espionage.''}}
%
%    %\item \only<-3>{\small{
%%    He left GM for VW in March 1993.}}
%% \only<4->{\small{
%%    \SoulColor\hl{He left GM for VW} in March 1993.}}
%\end{itemize}
%}
%
%\end{frame}
%
%\begin{frame}[t]{Factoids and Pyramids}
%    \textbf{Factoids} \cite{teufel2004evaluating} \\
%    \textbf{Pyramid} \cite{nenkova2007pyramid}
%    \begin{itemize}
%        \item \small{Label factoids/SCUs in ref. summaries}
%        \item \small{Weight factoids/SCUs by frequency in 
%            references }
%        \item \small{Summary score $\propto$ to the sum of 
%            weights of factoids/SCUs contained}
%    \end{itemize}
%
%    Disagreement in annotator stability:
%
%    \begin{itemize}
%            \only<1>{
%        \small{\item Factoids require 20-30 reference 
%                summaries\\ 
%            \cite{teufel2004evaluating}}
%        }
%            \only<2->{
%        \small{\item \textcolor{red}{Factoids require 20-30 reference 
%                summaries}\\ 
%            \cite{teufel2004evaluating}}
%        }
%        \only<-2>{
%        \small{\item Pyramid requires at least 5 reference summaries\\ 
%            \cite{nenkova2007pyramid}}
%        \small{\item Pyramid requires at least 2-3 reference summaries for 
%                correlation with responsiveness\\ 
%        \cite{owczarzak2009evaluation}}}
%        \only<3>{
%        \small{\item \textcolor{green}{Pyramid requires at least 5 reference 
%                summaries}\\ 
%            \cite{nenkova2007pyramid}}
%        \small{\item \textcolor{green}{Pyramid requires at least 2-3 reference
%                    summaries for 
%                correlation with responsiveness}\\ 
%        \cite{owczarzak2009evaluation}}}
%
%    \end{itemize}
%
%\end{frame}
%
%\begin{frame}{Content Evaluation Road Map}
% \begin{tikzpicture}[
%    dep/.style={draw, shape=circle, ultra thick, fill=blue!20},
%    depa/.style={draw, shape=circle, ultra thick, fill=green},
%]
%
%\draw[fill=none,draw=none] (-2,1.6) rectangle (8,-5.3);
%\draw[fill=blue!5,draw=none] (-2,1.6) rectangle (8,-1.8);
%\draw[fill=red!5,draw=none] (-2,-5.3) rectangle (8,-1.8);
%\draw[dashed] (-2,-1.8) -- (8,-1.8);
%\node (a) at (0,0) {\textbf{Shallow Coverage}};
%\node[align=center] (b) at (6.2,-3.0) 
%    {\textbf{Semantically}\\ \textbf{Informed Coverage}};
%
%   \draw[thick,dotted] (4.9,0.6) rectangle (7.5,-1.1);
%\draw[thick,dotted] (-1.6,-1.9) rectangle (1.6,-5.3);
%\draw[thick,dotted] (1.7,1.6) rectangle (4.3,-4.3);
%    \node[align=center] (x) at (0,-5) {\small \textbf{Needs Reference}\\
%        \small \textbf{Human Annotation}\\};
%    \node[align=center] (y) at (3,1) {\small \textbf{Reference Only}\\};
%    \node[align=center] (z) at (6.2,0) {\small \textbf{Reference Free}\\};
%
%    \node[dep] (linnode) at (3,.2) {};
%    \node[] (lintxt) at (3,-.2) {\tiny \cite{lin2004rouge}};
%    \node[dep] (conroynode) at (3,-1) {};
%    \node[] (conroytxt) at (3,-1.4) {\tiny \cite{conroy2011nouveau}};
%
%    %\path [->] (A) edge (B); 
%
%    \node[dep] (teufelnode) at (0,-2.2) {};
%    \node[] (teufeltxt) at (0,-2.6) {\tiny \cite{teufel2004evaluating}};
%    \node[dep] (nenkovanode) at (0,-3.2) {};
%    \node[] (nenkovatxt) at (0,-3.6) {\tiny \cite{nenkova2007pyramid}};
%
%\visible<1>{
%    \node[depa] (teufelnode) at (0,-2.2) {};
%    \node[depa] (nenkovanode) at (0,-3.2) {};
%\draw[thick,draw=green] (-1.6,-1.9) rectangle (1.6,-5.3);
%\node[] (teufeltxt) at (0,-2.6) {\tiny\textcolor{green}{ \cite{teufel2004evaluating}}};
%\node[] (nenkovatxt) at (0,-3.6) {\tiny\textcolor{green}{\cite{nenkova2007pyramid}}};
%
%\node[align=center] (x) at (0,-5) {\small \textcolor{green}{\textbf{Needs Reference}}\\
%\small \textcolor{green}{\textbf{Human Annotation}}\\};
%}
%
%\visible<3>{
%    \node[depa] (linnode) at (3,.2) {};
%    \node[] (lintxt) at (3,-.2) {\tiny \textcolor{green}{\cite{lin2004rouge}}};
%    \node[depa] (conroynode) at (3,-1) {};
%    \node[] (conroytxt) at (3,-1.4) {\tiny \textcolor{green}{\cite{conroy2011nouveau}}};
%
%    \node[align=center] (y) at (3,1) {\small \textcolor{green}{\textbf{Reference Only}}\\};
%\draw[thick,draw=green] (1.7,1.6) rectangle (4.3,-1.8);
%}
%
%
%    \node[dep] (hennignode) at (3,-2.7) {};
%    \node[] (hennigtxt) at (3,-3.1) {\tiny \cite{hennig2010learning}};
%    \node[dep] (yangnode) at (3,-3.7) {};
%    \node[] (yangtxt) at (3,-4.1) {\tiny \cite{yang2016peak}};
%
%    \node[dep] (louisnode) at (6.2,-.5) {};
%    \node[] (louistxt) at (6.2, -.9) {\tiny \cite{louis2009automatically}};
%    \path [->] (nenkovanode) edge (hennignode); 
%    \path [->] (nenkovanode) edge (yangnode); 
%
%\end{tikzpicture}
%
%\end{frame}
%
%\begin{frame}{Reference Only, Shallow Coverage}
%
%\textbf{ROUGE }\cite{lin2004rouge} -- recall of reference summary ngrams
%\begin{itemize}
%        \small
%    \item de facto standard evaluation tool outside of DUC/TAC
%    \item insensitive to lexical variation
%    %\item uniformly rewards sequence matches, e.g.``of the'' probably not indicative of anything
%    %\item insensitive to linguistic quality, e.g. coherence or discourse
%    \item \only<1>{correlation with coverage is only strong at 
%        200 and 400 word length} 
%\only<2->{\textcolor{red}{correlation with coverage is only strong at 200 and 400 word length}}
%        {\tiny \cite{lin2004rouge,nenkova2005automatic}}
%    \item \only<-2>{Strong rank correlation with respons.  and pyramid 
%        (R2 $>$ .87)!}
%        \only<3->{\textcolor{green}{Strong rank correlation with respons. 
%            and pyramid (R2 $>$ .87)!}}
%    {\tiny \cite{louis2009automatically,owczarzak2009evaluation}}
%\end{itemize}
%\textbf{Nouveau ROUGE} \cite{conroy2011nouveau} -- {\small{(Update Summarization)}} penalize for high ROUGE score
%on background set summary 
%\begin{itemize}
%        \small
%    \item \only<-3>{Makes relationship between ROUGE and pyramid/respons. 
%        more linear}
%        \only<4>{\textcolor{green}{Makes relationship between ROUGE and 
%                pyramid/respons. more linear}}
%
%\end{itemize}
%\end{frame}
%
%
%\begin{frame}{Content Evaluation Road Map}
% \begin{tikzpicture}[
%    dep/.style={draw, shape=circle, ultra thick, fill=blue!20},
%    depa/.style={draw, shape=circle, ultra thick, fill=green},
%]
%
%\draw[fill=none,draw=none] (-2,1.6) rectangle (8,-5.3);
%\draw[fill=blue!5,draw=none] (-2,1.6) rectangle (8,-1.8);
%\draw[fill=red!5,draw=none] (-2,-5.3) rectangle (8,-1.8);
%\draw[dashed] (-2,-1.8) -- (8,-1.8);
%\node (a) at (0,0) {\textbf{Shallow Coverage}};
%\node[align=center] (b) at (6.2,-3.0) 
%    {\textbf{Semantically}\\ \textbf{Informed Coverage}};
%
%   \draw[thick,dotted] (4.9,0.6) rectangle (7.5,-1.1);
%\draw[thick,dotted] (-1.6,-1.9) rectangle (1.6,-5.3);
%\draw[thick,dotted] (1.7,1.6) rectangle (4.3,-4.3);
%    \node[align=center] (x) at (0,-5) {\small \textbf{Needs Reference}\\
%        \small \textbf{Human Annotation}\\};
%    \node[align=center] (y) at (3,1) {\small \textbf{Reference Only}\\};
%    \node[align=center] (z) at (6.2,0) {\small \textbf{Reference Free}\\};
%
%    \node[dep] (linnode) at (3,.2) {};
%    \node[] (lintxt) at (3,-.2) {\tiny \cite{lin2004rouge}};
%    \node[dep] (conroynode) at (3,-1) {};
%    \node[] (conroytxt) at (3,-1.4) {\tiny \cite{conroy2011nouveau}};
%
%    %\path [->] (A) edge (B); 
%
%    \node[dep] (teufelnode) at (0,-2.2) {};
%    \node[] (teufeltxt) at (0,-2.6) {\tiny \cite{teufel2004evaluating}};
%    \node[dep] (nenkovanode) at (0,-3.2) {};
%    \node[] (nenkovatxt) at (0,-3.6) {\tiny \cite{nenkova2007pyramid}};
%    \node[dep] (hennignode) at (3,-2.7) {};
%    \node[] (hennigtxt) at (3,-3.1) {\tiny \cite{hennig2010learning}};
%    \node[dep] (yangnode) at (3,-3.7) {};
%    \node[] (yangtxt) at (3,-4.1) {\tiny \cite{yang2016peak}};
%
%    \node[dep] (louisnode) at (6.2,-.5) {};
%    \node[] (louistxt) at (6.2, -.9) {\tiny \cite{louis2009automatically}};
%    \path [->] (nenkovanode) edge (hennignode); 
%    \path [->] (nenkovanode) edge (yangnode); 
%
%\
%
%\visible<1>{
%    \node[depa] (linnode) at (3,.2) {};
%    \node[] (lintxt) at (3,-.2) {\tiny \textcolor{green}{\cite{lin2004rouge}}};
%    \node[depa] (conroynode) at (3,-1) {};
%    \node[] (conroytxt) at (3,-1.4) {\tiny \textcolor{green}{\cite{conroy2011nouveau}}};
%
%    \node[align=center] (y) at (3,1) {\small \textcolor{green}{\textbf{Reference Only}}\\};
%\draw[thick,draw=green] (1.7,1.6) rectangle (4.3,-1.8);
%}
%
%\visible<3>{
%    \node[depa] (hennignode) at (3,-2.7) {};
%    \node[] (hennigtxt) at (3,-3.1) {\tiny \textcolor{green}{\cite{hennig2010learning}}};
%    \node[depa] (yangnode) at (3,-3.7) {};
%    \node[] (yangtxt) at (3,-4.1) {\tiny \textcolor{green}{\cite{yang2016peak}}};
%
%
%    \node[align=center] (y) at (3,1) {\small \textcolor{green}{\textbf{Reference Only}}\\};
%\draw[thick,draw=green] (1.7,-1.8) rectangle (4.3,-4.3);
%}
%
%\end{tikzpicture}
%
%\end{frame}
%
%\begin{frame}{Automating Pyramid}
%
%    \textbf{LDA} \cite{hennig2010learning}
%\begin{itemize}
%        \small
%\item LDA topics and SCU's unigram distributions are very similar 
%\item \only<1>{good matching between LDA topics and SCUs is possible}
%    \only<2->{\textcolor{green}{good matching between LDA topics and SCUs is 
%        possible}}
%\item \only<-2>{theoretical, no method yet of selecting appropriate 
%        topics w/o SCUs}
%    \only<3->{\textcolor{red}{theoretical, no method yet of selecting 
%        appropriate topics w/o SCUs}}
%\end{itemize}
%
%\textbf{PEAK} \cite{yang2016peak}
%\begin{itemize}
%        \small
%\item Uses Open-IE style pattern extraction as stand-in for SCU's
%\item System SCU's mapped to Reference SCU's via maximal matching %(Munkres-Kuhn algo.)
%\item \only<-3>{(DUC 06) moderate correlation with pyramid scores (.7094)}
%    \only<4->{\textcolor{green}{(DUC 06) moderate correlation with pyramid 
%        scores (.7094)}}
%\item \only<-4>{unclear if the rank correlation is higher than ROUGE-2}
%    \only<5>{\textcolor{red}{unclear if the rank correlation is higher than 
%        ROUGE-2}}
%\end{itemize}
%\end{frame}
%
%\begin{frame}{Content Evaluation Road Map}
% \begin{tikzpicture}[
%    dep/.style={draw, shape=circle, ultra thick, fill=blue!20},
%    depa/.style={draw, shape=circle, ultra thick, fill=green},
%]
%
%\draw[fill=none,draw=none] (-2,1.6) rectangle (8,-5.3);
%\draw[fill=blue!5,draw=none] (-2,1.6) rectangle (8,-1.8);
%\draw[fill=red!5,draw=none] (-2,-5.3) rectangle (8,-1.8);
%\draw[dashed] (-2,-1.8) -- (8,-1.8);
%\node (a) at (0,0) {\textbf{Shallow Coverage}};
%\node[align=center] (b) at (6.2,-3.0) 
%    {\textbf{Semantically}\\ \textbf{Informed Coverage}};
%
%   \draw[thick,dotted] (4.9,0.6) rectangle (7.5,-1.1);
%\draw[thick,dotted] (-1.6,-1.9) rectangle (1.6,-5.3);
%\draw[thick,dotted] (1.7,1.6) rectangle (4.3,-4.3);
%    \node[align=center] (x) at (0,-5) {\small \textbf{Needs Reference}\\
%        \small \textbf{Human Annotation}\\};
%    \node[align=center] (y) at (3,1) {\small \textbf{Reference Only}\\};
%    \node[align=center] (z) at (6.2,0) {\small \textbf{Reference Free}\\};
%
%    \node[dep] (linnode) at (3,.2) {};
%    \node[] (lintxt) at (3,-.2) {\tiny \cite{lin2004rouge}};
%    \node[dep] (conroynode) at (3,-1) {};
%    \node[] (conroytxt) at (3,-1.4) {\tiny \cite{conroy2011nouveau}};
%
%    %\path [->] (A) edge (B); 
%
%    \node[dep] (teufelnode) at (0,-2.2) {};
%    \node[] (teufeltxt) at (0,-2.6) {\tiny \cite{teufel2004evaluating}};
%    \node[dep] (nenkovanode) at (0,-3.2) {};
%    \node[] (nenkovatxt) at (0,-3.6) {\tiny \cite{nenkova2007pyramid}};
%    \node[dep] (hennignode) at (3,-2.7) {};
%    \node[] (hennigtxt) at (3,-3.1) {\tiny \cite{hennig2010learning}};
%    \node[dep] (yangnode) at (3,-3.7) {};
%    \node[] (yangtxt) at (3,-4.1) {\tiny \cite{yang2016peak}};
%
%    \node[dep] (louisnode) at (6.2,-.5) {};
%    \node[] (louistxt) at (6.2, -.9) {\tiny \cite{louis2009automatically}};
%    \path [->] (nenkovanode) edge (hennignode); 
%    \path [->] (nenkovanode) edge (yangnode); 
%
%\
%
%
%\visible<1>{
%  \node[depa] (hennignode) at (3,-2.7) {};
%  \node[] (hennigtxt) at (3,-3.1) {\tiny \textcolor{green}{\cite{hennig2010learning}}};
%  \node[depa] (yangnode) at (3,-3.7) {};
%  \node[] (yangtxt) at (3,-4.1) {\tiny \textcolor{green}{\cite{yang2016peak}}};
%  \node[align=center] (y) at (3,1) {
%      \small \textcolor{green}{\textbf{Reference Only}}\\};
%  \draw[thick,draw=green] (1.7,-1.8) rectangle (4.3,-4.3);
%}
%\visible<3>{
%    \node[depa] (louisnode) at (6.2,-.5) {};
%    \node[] (louistxt) at (6.2, -.9) {\tiny \textcolor{green}{\cite{louis2009automatically}}};
%   \draw[thick,draw=green] (4.9,0.6) rectangle (7.5,-1.1);
%  \node[align=center] (z) at (6.2,0) {\small \textcolor{green}{\textbf{Reference Free}}\\};
%}
%\end{tikzpicture}
%
%\end{frame}
%
%
%
%\begin{frame}{Evaluation without Human Models}
%\cite{louis2009automatically} -- examines correlation between input/summary 
%    features and TAC08 responsiveness and pyramid scores.
%\begin{itemize}
%        \small
%    \item (across systems) Jensen-Shannon (JS) divergence highly correlated 
%        with metrics \\
%        Pyramid $-.88\;\;\;\;\;$ Respons. $-.74$  
% %$\displaystyle JS(S||I) = \frac{1}{2}\Big(KL(S||I) + KL(I||S) \Big)$
%\item (within topics) correlations vary wildly per input \\
%\item \only<1>{establishing system rankings seems possible w/o references}
%    \only<2->{\textcolor{green}{establishing system rankings seems possible 
%        w/o references}}
%\item \only<-2>{(query-focused) high correlation between summary/inputs 
%        w/o using query}
%    \only<3>{\textcolor{red}{(query-focused) high correlation between 
%        summary/inputs w/o using query}}
%%\item (update) using background documents results in worse correlation
%\end{itemize}
%\end{frame}
%
%
%
%
%
%\section{Extraction}
%
%
%\begin{frame}{Extractive Summarization}
%    Construct a summary by selecting a subset of sentences (or words) from 
%    the inputs.
%\end{frame}
%
%\begin{frame}{Extractive Summarization}
%    Why is extractive summarization so popular?
%    \begin{itemize}
%        \item Text generation is hard!
%        \item Semantic representations are brittle, low coverage. 
%        \item Surface level features (i.e. words) have straightforward 
%            correspondence to ROUGE and human coverage metrics
%    \end{itemize}
%\end{frame}
%
%
%
%%\begin{frame}{Overview}
%%Ranking Sentences By Term Importance
%%\begin{itemize}
%%    \item Topic Signatures \cite{lin2000automated,conroy2005classy}
%%    \item Topic Models \cite{titov2008joint}
%%    \item Ngram Coverage \cite{gillick2009global,lin2011class,liu2011sxsw}
%%    \item Graph Centrality \cite{erkan2004lexrank}
%%    \item Structural/Discourse Features \cite{maskey2005comparing}
%%\end{itemize}
%%\end{frame}
%
%\begin{frame}{Summarization in Different Domains}
%    \uncover<2>{\textbf{Retrospective Summarization}}
%    \begin{itemize}
%        \item[] \includegraphics[scale=.017]{news_icon}~ \textbf{News} 
%    \begin{itemize}
%        \item Text {\tiny\cite{lin2000automated,erkan2004lexrank,conroy2005classy,lin2011class}}
%        \item Broadcast/Speech {\tiny\cite{maskey2005comparing}}
%    \end{itemize}
%\item[] \includegraphics[scale=.02]{rating_icon}~ \textbf{Opinions/Reviews} {\tiny\cite{titov2008joint}}
%\item[] \includegraphics[scale=.02]{meeting_icon}~ \textbf{Meetings  } {\tiny\cite{gillick2009global}}
%\item[] \includegraphics[scale=.02]{tweet_icon}~ \textbf{Microblogs/Twitter} {\tiny\cite{liu2011sxsw}}
%    \end{itemize}
%    \uncover<2>{
%        \textbf{Streaming Summarization}
%    \begin{itemize}
%        \item[] \includegraphics[scale=.017]{news_icon}~ \textbf{News text} {\tiny\cite{guo2013updating,mccreadie2014incremental}}
%    \end{itemize}
%    }
%\end{frame}

\begin{frame}{Extractive Summarization}
  \begin{figure}[!h]
    \centering 
    \begin{tikzpicture}
 
      \node[] (C) at (4.7,-2) {Ranking};
      \draw[opacity=.2] (4.7,-.5) circle (2.0cm);
      \node (C) at (4.7,0) {\includegraphics[scale=.017]{news_icon}};
      \node (C) at (4.7,-.3) {\tiny \cite{lin2000automated}};
      \node (C) at (4.7,-.6) {\tiny \cite{erkan2004lexrank}};
         
      \node[] (C) at (6.2,4.5) {Regression};
      \draw[opacity=.2] (6.1,3.2) circle (1.8cm);
      \node (C) at (6.1,3.5) {\includegraphics[scale=.017]{news_icon}};
      \node (C) at (6.1,3.2) {\tiny \cite{guo2013updating}};
      \node (C) at (6.1,2.9) {\tiny \cite{mccreadie2014incremental}};
     
      \node (C) at (1.5,5) {Classification};
      \draw[opacity=.2] (1.5,3) circle (2.5cm);
      \node (C) at (1.5,4) {\includegraphics[scale=.017]{news_icon}};
      \node (C) at (1.5,3.7) {\tiny \cite{maskey2005comparing}};
      \node (C) at (1.5,3.3) {\includegraphics[scale=.022]{rating_icon}};
      \node (C) at (1.5,3.0) {\tiny \cite{titov2008joint}};
     
      \node (C) at (.7,1.7) {\includegraphics[scale=.017]{news_icon}};
      \node (C) at (.7,1.3) {\tiny \cite{conroy2005classy}};
     
      \node[] (C) at (0,-2) {Coverage Opt.};
      \draw[opacity=.2] (0,0) circle (2.5cm);
      \node (C) at (-1.0,.4) {\includegraphics[scale=.017]{tweet_icon}};
      \node (C) at (-1.0,.1) {\tiny \cite{liu2011sxsw}};
      \node (C) at (-1,-.4) {\includegraphics[scale=.022]{meeting_icon}};
      \node (C) at (-1,-.7) {\tiny \cite{gillick2009global}};
      \node (C) at (1.1,0) {\includegraphics[scale=.017]{news_icon}};
      \node (C) at (1.1,-.3) {\tiny \cite{lin2011class}};
      
   \end{tikzpicture}
 \end{figure}
\end{frame}

\begin{frame}{Extraction by Ranking}
  \begin{figure}[!h]
    \centering 
    \begin{tikzpicture}
 
      \node[text=red,opacity=.7] (C) at (4.7,-2) {Ranking};
      \draw[opacity=.2] (4.7,-.5) circle (2.0cm);
      \node at (4.7,0) {\includegraphics[scale=.022]{news_icon}};
      \node[text=red,opacity=.7] at (4.7,-.3) {\tiny \cite{lin2000automated}};
      \node[text=red,opacity=.7] at (4.7,-.6) {\tiny \cite{erkan2004lexrank}};
         
      \node[] (C) at (6.2,4.5) {Regression};
      \draw[opacity=.2] (6.1,3.2) circle (1.8cm);
      \node (C) at (6.1,3.5) {\includegraphics[scale=.017]{news_icon}};
      \node (C) at (6.1,3.2) {\tiny \cite{guo2013updating}};
      \node (C) at (6.1,2.9) {\tiny \cite{mccreadie2014incremental}};
     
      \node (C) at (1.5,5) {Classification};
      \draw[opacity=.2] (1.5,3) circle (2.5cm);
      \node (C) at (1.5,4) {\includegraphics[scale=.017]{news_icon}};
      \node (C) at (1.5,3.7) {\tiny \cite{maskey2005comparing}};
      \node (C) at (1.5,3.3) {\includegraphics[scale=.022]{rating_icon}};
      \node (C) at (1.5,3.0) {\tiny \cite{titov2008joint}};
     
      \node (C) at (.7,1.7) {\includegraphics[scale=.017]{news_icon}};
      \node (C) at (.7,1.3) {\tiny \cite{conroy2005classy}};
     
      \node[] (C) at (0,-2) {Coverage Opt.};
      \draw[opacity=.2] (0,0) circle (2.5cm);
      \node (C) at (-1.0,.4) {\includegraphics[scale=.017]{tweet_icon}};
      \node (C) at (-1.0,.1) {\tiny \cite{liu2011sxsw}};
      \node (C) at (-1,-.4) {\includegraphics[scale=.022]{meeting_icon}};
      \node (C) at (-1,-.7) {\tiny \cite{gillick2009global}};
      \node (C) at (1.1,0) {\includegraphics[scale=.017]{news_icon}};
      \node (C) at (1.1,-.3) {\tiny \cite{lin2011class}};
      
   \end{tikzpicture}
 \end{figure}
\end{frame}

\begin{frame}{Extraction by Regression}
  \begin{figure}[!h]
    \centering 
    \begin{tikzpicture}
 
      \visible<1>{  
       \node[text=red,opacity=.7] (C) at (4.7,-2) {Ranking};
       \draw[opacity=.2] (4.7,-.5) circle (2.0cm);
       \node at (4.7,0) {\includegraphics[scale=.022]{news_icon}};
       \node[text=red,opacity=.7] at (4.7,-.3) {\tiny \cite{lin2000automated}};
       \node[text=red,opacity=.7] at (4.7,-.6) {\tiny \cite{erkan2004lexrank}};
      }

      \visible<2->{  
       \node at (4.7,-2) {Ranking};
       \draw[opacity=.2] (4.7,-.5) circle (2.0cm);
       \node at (4.7,0) {\includegraphics[scale=.017]{news_icon}};
       \node at (4.7,-.3) {\tiny \cite{lin2000automated}};
       \node at (4.7,-.6) {\tiny \cite{erkan2004lexrank}};
      }
      \visible<-2>{
       \node[] (C) at (6.2,4.5) {Regression};
       \draw[opacity=.2] (6.1,3.2) circle (1.8cm);
       \node (C) at (6.1,3.5) {\includegraphics[scale=.017]{news_icon}};
       \node (C) at (6.1,3.2) {\tiny \cite{guo2013updating}};
       \node (C) at (6.1,2.9) {\tiny \cite{mccreadie2014incremental}};
      }

      \visible<3>{
       \node[text=red,opacity=.7] (C) at (6.2,4.5) {Regression};
       \draw[opacity=.2] (6.1,3.2) circle (1.8cm);
       \node (C) at (6.1,3.5) {\includegraphics[scale=.022]{news_icon}};
       \node[text=red,opacity=.7] at (6.1,3.2) {\tiny \cite{guo2013updating}};
       \node[text=red,opacity=.7] at (6.1,2.9) 
            {\tiny \cite{mccreadie2014incremental}};
      }
      
      \node (C) at (1.5,5) {Classification};
      \draw[opacity=.2] (1.5,3) circle (2.5cm);
      \node (C) at (1.5,4) {\includegraphics[scale=.017]{news_icon}};
      \node (C) at (1.5,3.7) {\tiny \cite{maskey2005comparing}};
      \node (C) at (1.5,3.3) {\includegraphics[scale=.022]{rating_icon}};
      \node (C) at (1.5,3.0) {\tiny \cite{titov2008joint}};
     
      \node (C) at (.7,1.7) {\includegraphics[scale=.017]{news_icon}};
      \node (C) at (.7,1.3) {\tiny \cite{conroy2005classy}};
     
      \node[] (C) at (0,-2) {Coverage Opt.};
      \draw[opacity=.2] (0,0) circle (2.5cm);
      \node (C) at (-1.0,.4) {\includegraphics[scale=.017]{tweet_icon}};
      \node (C) at (-1.0,.1) {\tiny \cite{liu2011sxsw}};
      \node (C) at (-1,-.4) {\includegraphics[scale=.022]{meeting_icon}};
      \node (C) at (-1,-.7) {\tiny \cite{gillick2009global}};
      \node (C) at (1.1,0) {\includegraphics[scale=.017]{news_icon}};
      \node (C) at (1.1,-.3) {\tiny \cite{lin2011class}};
      
   \end{tikzpicture}
 \end{figure}
\end{frame}

\begin{frame}{Extraction by Classification}
  \begin{figure}[!h]
    \centering 
    \begin{tikzpicture}
 
      \node[] (C) at (4.7,-2) {Ranking};
      \draw[opacity=.2] (4.7,-.5) circle (2.0cm);
      \node (C) at (4.7,0) {\includegraphics[scale=.017]{news_icon}};
      \node (C) at (4.7,-.3) {\tiny \cite{lin2000automated}};
      \node (C) at (4.7,-.6) {\tiny \cite{erkan2004lexrank}};
      
      \visible<1>{
       \node[text=red,opacity=.7] at (6.2,4.5) {Regression};
       \draw[opacity=.2] (6.1,3.2) circle (1.8cm);
       \node at (6.1,3.5) {\includegraphics[scale=.022]{news_icon}};
       \node[text=red,opacity=.7] at (6.1,3.2) {\tiny \cite{guo2013updating}};
       \node[text=red,opacity=.7] at (6.1,2.9) 
            {\tiny \cite{mccreadie2014incremental}};
      }
      \visible<2->{
       \node at (6.2,4.5) {Regression};
       \draw[opacity=.2] (6.1,3.2) circle (1.8cm);
       \node at (6.1,3.5) {\includegraphics[scale=.017]{news_icon}};
       \node at (6.1,3.2) {\tiny \cite{guo2013updating}};
       \node at (6.1,2.9) {\tiny \cite{mccreadie2014incremental}};
      }
      
      \visible<-2>{
       \node at (1.5,5) {Classification};
       \draw[opacity=.2] (1.5,3) circle (2.5cm);
       \node at (1.5,4) {\includegraphics[scale=.017]{news_icon}};
       \node at (1.5,3.7) {\tiny \cite{maskey2005comparing}};
       \node at (1.5,3.3) {\includegraphics[scale=.022]{rating_icon}};
       \node at (1.5,3.0) {\tiny \cite{titov2008joint}};
     
       \node at (.7,1.7) {\includegraphics[scale=.017]{news_icon}};
       \node at (.7,1.3) {\tiny \cite{conroy2005classy}};
      }
      \visible<3>{
       \node[text=red,opacity=.7] at (1.5,5) {Classification};
       \draw[opacity=.2] (1.5,3) circle (2.5cm);
       \node at (1.5,4) {\includegraphics[scale=.022]{news_icon}};
       \node[text=red,opacity=.7] at (1.5,3.7) 
            {\tiny \cite{maskey2005comparing}};
       \node at (1.5,3.3) {\includegraphics[scale=.025]{rating_icon}};
       \node[text=red,opacity=.7] at (1.5,3.0) {\tiny \cite{titov2008joint}};
     
       \node (C) at (.7,1.7) {\includegraphics[scale=.022]{news_icon}};
       \node[text=red,opacity=.7] at (.7,1.3) {\tiny \cite{conroy2005classy}};
      }

      \node[] (C) at (0,-2) {Coverage Opt.};
      \draw[opacity=.2] (0,0) circle (2.5cm);
      \node (C) at (-1.0,.4) {\includegraphics[scale=.017]{tweet_icon}};
      \node (C) at (-1.0,.1) {\tiny \cite{liu2011sxsw}};
      \node (C) at (-1,-.4) {\includegraphics[scale=.022]{meeting_icon}};
      \node (C) at (-1,-.7) {\tiny \cite{gillick2009global}};
      \node (C) at (1.1,0) {\includegraphics[scale=.017]{news_icon}};
      \node (C) at (1.1,-.3) {\tiny \cite{lin2011class}};
      
   \end{tikzpicture}
 \end{figure}
\end{frame}

\begin{frame}{Extraction by Coverage Optimization}
  \begin{figure}[!h]
    \centering 
    \begin{tikzpicture}
 
      \node[] (C) at (4.7,-2) {Ranking};
      \draw[opacity=.2] (4.7,-.5) circle (2.0cm);
      \node (C) at (4.7,0) {\includegraphics[scale=.017]{news_icon}};
      \node (C) at (4.7,-.3) {\tiny \cite{lin2000automated}};
      \node (C) at (4.7,-.6) {\tiny \cite{erkan2004lexrank}};
         
      \node[] (C) at (6.2,4.5) {Regression};
      \draw[opacity=.2] (6.1,3.2) circle (1.8cm);
      \node (C) at (6.1,3.5) {\includegraphics[scale=.017]{news_icon}};
      \node (C) at (6.1,3.2) {\tiny \cite{guo2013updating}};
      \node (C) at (6.1,2.9) {\tiny \cite{mccreadie2014incremental}};

      \visible<1>{
       \node[text=red,opacity=.7] at (1.5,5) {Classification};
       \draw[opacity=.2] (1.5,3) circle (2.5cm);
       \node at (1.5,4) {\includegraphics[scale=.022]{news_icon}};
       \node[text=red,opacity=.7] at (1.5,3.7) 
            {\tiny \cite{maskey2005comparing}};
       \node at (1.5,3.3) {\includegraphics[scale=.025]{rating_icon}};
       \node[text=red,opacity=.7] at (1.5,3.0) {\tiny \cite{titov2008joint}};
     
       \node (C) at (.7,1.7) {\includegraphics[scale=.022]{news_icon}};
       \node[text=red,opacity=.7] at (.7,1.3) {\tiny \cite{conroy2005classy}};
      }

     
      \visible<2->{
       \node (C) at (1.5,5) {Classification};
       \draw[opacity=.2] (1.5,3) circle (2.5cm);
       \node (C) at (1.5,4) {\includegraphics[scale=.017]{news_icon}};
       \node (C) at (1.5,3.7) {\tiny \cite{maskey2005comparing}};
       \node (C) at (1.5,3.3) {\includegraphics[scale=.022]{rating_icon}};
       \node (C) at (1.5,3.0) {\tiny \cite{titov2008joint}};
      }
      \visible<2>{
       \node (C) at (.7,1.7) {\includegraphics[scale=.017]{news_icon}};
       \node (C) at (.7,1.3) {\tiny \cite{conroy2005classy}};
      }
      \visible<3->{
       \node at (.7,1.7) {\includegraphics[scale=.022]{news_icon}};
       \node[text=red,opacity=.7] at (.7,1.3) {\tiny \cite{conroy2005classy}};
      }

   
      \visible<-2>{
       \node[] (C) at (0,-2) {Coverage Opt.};
       \draw[opacity=.2] (0,0) circle (2.5cm);
       \node (C) at (-1.0,.4) {\includegraphics[scale=.017]{tweet_icon}};
       \node (C) at (-1.0,.1) {\tiny \cite{liu2011sxsw}};
       \node (C) at (-1,-.4) {\includegraphics[scale=.022]{meeting_icon}};
       \node (C) at (-1,-.7) {\tiny \cite{gillick2009global}};
       \node (C) at (1.1,0) {\includegraphics[scale=.017]{news_icon}};
       \node (C) at (1.1,-.3) {\tiny \cite{lin2011class}};
      }
      \visible<3->{
       \node[text=red,opacity=.7] (C) at (0,-2) {Coverage Opt.};
       \draw[opacity=.2] (0,0) circle (2.5cm);
       \node (C) at (-1.0,.4) {\includegraphics[scale=.022]{tweet_icon}};
       \node[text=red,opacity=.7] at (-1.0,.1) {\tiny \cite{liu2011sxsw}};
       \node at (-1,-.4) {\includegraphics[scale=.025]{meeting_icon}};
       \node[text=red,opacity=.7] at (-1,-.7) {\tiny \cite{gillick2009global}};
       \node at (1.1,0) {\includegraphics[scale=.022]{news_icon}};
       \node[text=red,opacity=.7] at (1.1,-.3) {\tiny \cite{lin2011class}};
      }
   \end{tikzpicture}
 \end{figure}
\end{frame}




\begin{frame}{Extraction by Ranking}
    \begin{figure}[!h]
\centering
 \begin{tikzpicture}
     \node (C) at (1.5,5) {Classification};
     \node[rotate=45,text=red,opacity=.7] (C) at (4.5,-1.7) {Ranking};
     \node[rotate=-45] (C) at (-1.5,-1.7) {Coverage Opt.};
     \node (C) at (1.5,4) {\includegraphics[scale=.017]{news_icon}};
     \node (C) at (1.5,3.7) {\tiny \cite{maskey2005comparing}};
     \node (C) at (1.5,3.3) {\includegraphics[scale=.022]{rating_icon}};
     \node (C) at (1.5,3.0) {\tiny \cite{titov2008joint}};
     \node (C) at (0.3,-1.8) {\includegraphics[scale=.017]{news_icon}};
     \node (C) at (0.3,-2.1) {\tiny \cite{lin2011class}};
     \node (C) at (-1.8,.3) {\includegraphics[scale=.017]{tweet_icon}};
     \node (C) at (-1.8,0) {\tiny \cite{liu2011sxsw}};
     \node (C) at (-.8,-.6) {\includegraphics[scale=.022]{meeting_icon}};
     \node (C) at (-.8,-.9) {\tiny \cite{gillick2009global}};
     \node (C) at (-.4,2.2) {\includegraphics[scale=.017]{news_icon}};
     \node (C) at (-.4,1.9) {\tiny \cite{conroy2005classy}};
     \node (C) at (4,.4) {\includegraphics[scale=.022]{news_icon}};
     \node[text=red,opacity=.7] (C) at (4,.1) {\tiny \cite{lin2000automated}};
     \node[text=red,opacity=.7] (C) at (4,-.2) {\tiny \cite{erkan2004lexrank}};
     \node[text=red,opacity=.7] (C) at (4,-.5) {\tiny \cite{guo2013updating}};
     \node[text=red,opacity=.7] (C) at (4,-.8) {\tiny \cite{mccreadie2014incremental}};

    \draw[opacity=.2] (0,0) circle (2.5cm);
    \draw[opacity=.2,color=red] (3,0) circle (2.5cm);
    \draw[opacity=.2] (1.5,3) circle (2.5cm);
 \end{tikzpicture}
 \end{figure}
\end{frame}



\begin{frame}{Extraction by Classification}

    \begin{figure}[!h]
        \centering
 \begin{tikzpicture}
     \node[text=red,opacity=.7] (C) at (1.5,5) {Classification};
     \node[rotate=45] (C) at (4.5,-1.7) {Ranking};
     \node[rotate=-45] (C) at (-1.5,-1.7) {Coverage Opt.};
     \node (C) at (1.5,4) {\includegraphics[scale=.022]{news_icon}};
     \node[text=red,opacity=.7] (C) at (1.5,3.7) {\tiny \cite{maskey2005comparing}};
     %\node (C) at (1.5,-1) {\includegraphics[scale=.017]{news_icon}};
     \node (C) at (0.3,-1.8) {\includegraphics[scale=.017]{news_icon}};
     \node (C) at (0.3,-2.1) {\tiny \cite{lin2011class}};
     \node (C) at (-1.8,.3) {\includegraphics[scale=.017]{tweet_icon}};
     \node (C) at (-1.8,0) {\tiny \cite{liu2011sxsw}};
     \node (C) at (-.8,-.6) {\includegraphics[scale=.022]{meeting_icon}};
     \node (C) at (-.8,-.9) {\tiny \cite{gillick2009global}};
     \node (C) at (-.4,2.2) {\includegraphics[scale=.022]{news_icon}};
     \node[text=red,opacity=.7] (C) at (-.4,1.9) {\tiny \cite{conroy2005classy}};
     \node (C) at (1.5,3.3) {\includegraphics[scale=.025]{rating_icon}};
     \node[text=red,opacity=.7] (C) at (1.5,3.0) {\tiny \cite{titov2008joint}};
     \node (C) at (4,.4) {\includegraphics[scale=.017]{news_icon}};
     \node (C) at (4,.1) {\tiny \cite{lin2000automated}};
     \node (C) at (4,-.2) {\tiny \cite{erkan2004lexrank}};
     \node (C) at (4,-.5) {\tiny \cite{guo2013updating}};
     \node (C) at (4,-.8) {\tiny \cite{mccreadie2014incremental}};
    \draw[opacity=.2] (0,0) circle (2.5cm);
    \draw[opacity=.2] (3,0) circle (2.5cm);
    \draw[opacity=.2,color=red] (1.5,3) circle (2.5cm);
 \end{tikzpicture}
 \end{figure}
\end{frame}

\begin{frame}{Extraction by Coverage Optimization}

    \begin{figure}[!h]
        \centering
 \begin{tikzpicture}
     \node (C) at (1.5,5) {Classification};
     \node[rotate=45] (C) at (4.5,-1.7) {Ranking};
     \node[rotate=-45,text=red,opacity=.7] (C) at (-1.5,-1.7) {Coverage Opt.};
     \node (C) at (1.5,4) {\includegraphics[scale=.017]{news_icon}};
     \node (C) at (1.5,3.7) {\tiny \cite{maskey2005comparing}};
     %\node (C) at (1.5,-1) {\includegraphics[scale=.017]{news_icon}};
     \node (C) at (0.3,-1.8) {\includegraphics[scale=.022]{news_icon}};
     \node[text=red,opacity=.7] (C) at (0.3,-2.1) {\tiny \cite{lin2011class}};
     \node (C) at (-1.8,.3) {\includegraphics[scale=.022]{tweet_icon}};
     \node[text=red,opacity=.7] (C) at (-1.8,0) {\tiny \cite{liu2011sxsw}};
     \node (C) at (-.8,-.6) {\includegraphics[scale=.025]{meeting_icon}};
     \node[text=red,opacity=.7] (C) at (-.8,-.9) {\tiny \cite{gillick2009global}};
     \node (C) at (-.4,2.2) {\includegraphics[scale=.022]{news_icon}};
     \node[text=red,opacity=.7] (C) at (-.4,1.9) {\tiny \cite{conroy2005classy}};
     \node (C) at (1.5,3.3) {\includegraphics[scale=.022]{rating_icon}};
     \node (C) at (1.5,3.0) {\tiny \cite{titov2008joint}};
     \node (C) at (4,.4) {\includegraphics[scale=.017]{news_icon}};
     \node (C) at (4,.1) {\tiny \cite{lin2000automated}};
     \node (C) at (4,-.2) {\tiny \cite{erkan2004lexrank}};
     \node (C) at (4,-.5) {\tiny \cite{guo2013updating}};
     \node (C) at (4,-.8) {\tiny \cite{mccreadie2014incremental}};
    \draw[opacity=.2,color=red] (0,0) circle (2.5cm);
    \draw[opacity=.2] (3,0) circle (2.5cm);
    \draw[opacity=.2] (1.5,3) circle (2.5cm);
 \end{tikzpicture}
 \end{figure}
\end{frame}




%\begin{frame}{Sentence Ranking}
%    Apply a ranking over sentences and add the top sentences until a length
%    constraint is reached.
%\end{frame}
%
%
%
%
%\begin{frame}{Sentence Ranking}
%    \begin{itemize}
%        \uncover<2->{\item \textbf{Centroid Similarity} cosine similarity of tf$\cdot$idf vector to the mean input tf$\cdot$idf vector  {\tiny \cite{erkan2004lexrank}}}
%        \uncover<3->{\item \textbf{Topic Signatures} rank sentences by number 
%            of topically important ngrams {\tiny\cite{lin2000automated}}}
%\uncover<3->{\begin{itemize}\item[] \small{\textit{topic signature} $\triangleq$ ngram unlikely to occur with such high frequency as determined by log-likelihood ratio} \end{itemize}}
%        \uncover<4->{\item \textbf{LexRank} rank sentences by graph centrality
%            {\tiny \cite{erkan2004lexrank}  }}
%\uncover<4->{\begin{itemize}\item[] \small{Sentences $\triangleq$ nodes; edges determined
%    by thresholded/weighted sent. similarity; rank computed with PageRank} \end{itemize}}
%
%    \end{itemize}
%\end{frame}
%
%\begin{frame}{Sentence Ranking}
%Potential Flaws:
%\begin{itemize}
%    \item Ranking methods typically need additional redundancy 
%        penalty/diversity reward
%    \item Regression on objective score more appropriate -- ranking implies 
%        consistent pairwise ordering (which definitely does not hold)
%\end{itemize}
%\end{frame}
%
%\begin{frame}{Sentence Ranking: Temporal Summarization}
%    \begin{itemize}
% \item \textbf{Regression w/ Cutoff} tune/predict rank cutoff using 
%        sentence level features, similarity features, previous sentence 
%        selection features
%        {\tiny \cite{guo2013updating,mccreadie2014incremental}  }
%    \end{itemize}
%\end{frame}
%
%\begin{frame}{Sentence Ranking: Temporal Summarization}
%    \cite{guo2013updating} -- add sentences at each time step that are likely
%    to improve ROUGE precision\\
%   
%\begin{itemize}
%    \item $P(s) \triangleq $ ROUGE Precision of sentence $s$
%    \item $\delta P(s|\mathcal{S}) \triangleq $ contribution to 
%        $P(\{s\}\cup\mathcal{S})$ of $s$ when added to summary $\mathcal{S}$
%\end{itemize}
%
%    \begin{enumerate}
%        \item \only<1>{filter sentences $s$ with predicted $P(s) < \tau_P$}
%            \only<2->{\sout{filter sentences $s$ with predicted $P(s) < \tau_P$}}\\\uncover<2->{take rank ordered output $\{s_1,\ldots, s_n \}$ of an extractive update summarizer }
%        \item \only<1-2>{add sentences $s$ with predicted $\delta P(s|\mathcal{S}) > \tau_{\delta P}$ to summary}
%    \only<3->{\sout{add sentences $s$ with predicted 
%    $\delta P(s|\mathcal{S}) > \tau_{\delta P}$ to summary}}
%    \uncover<3->{
%    Predict a rank cutoff $\theta$, s.t. $\{s_1,\ldots, s_\theta\}$ optimizes
%    evaluation metric
%    }
%    \end{enumerate}
%    \uncover<2->{*\cite{mccreadie2014incremental}}
%
%\end{frame}
%
%
%\begin{frame}{Extraction by Classification}
%
%    \begin{figure}[!h]
%        \centering
% \begin{tikzpicture}
%     \node[text=red,opacity=.7] (C) at (1.5,5) {Classification};
%     \node[rotate=45] (C) at (4.5,-1.7) {Ranking};
%     \node[rotate=-45] (C) at (-1.5,-1.7) {Coverage Opt.};
%     \node (C) at (1.5,4) {\includegraphics[scale=.022]{news_icon}};
%     \node[text=red,opacity=.7] (C) at (1.5,3.7) {\tiny \cite{maskey2005comparing}};
%     %\node (C) at (1.5,-1) {\includegraphics[scale=.017]{news_icon}};
%     \node (C) at (0.3,-1.8) {\includegraphics[scale=.017]{news_icon}};
%     \node (C) at (0.3,-2.1) {\tiny \cite{lin2011class}};
%     \node (C) at (-1.8,.3) {\includegraphics[scale=.017]{tweet_icon}};
%     \node (C) at (-1.8,0) {\tiny \cite{liu2011sxsw}};
%     \node (C) at (-.8,-.6) {\includegraphics[scale=.022]{meeting_icon}};
%     \node (C) at (-.8,-.9) {\tiny \cite{gillick2009global}};
%     \node (C) at (-.4,2.2) {\includegraphics[scale=.022]{news_icon}};
%     \node[text=red,opacity=.7] (C) at (-.4,1.9) {\tiny \cite{conroy2005classy}};
%     \node (C) at (1.5,3.3) {\includegraphics[scale=.025]{rating_icon}};
%     \node[text=red,opacity=.7] (C) at (1.5,3.0) {\tiny \cite{titov2008joint}};
%     \node (C) at (4,.4) {\includegraphics[scale=.017]{news_icon}};
%     \node (C) at (4,.1) {\tiny \cite{lin2000automated}};
%     \node (C) at (4,-.2) {\tiny \cite{erkan2004lexrank}};
%     \node (C) at (4,-.5) {\tiny \cite{guo2013updating}};
%     \node (C) at (4,-.8) {\tiny \cite{mccreadie2014incremental}};
%    \draw[opacity=.2] (0,0) circle (2.5cm);
%    \draw[opacity=.2] (3,0) circle (2.5cm);
%    \draw[opacity=.2,color=red] (1.5,3) circle (2.5cm);
% \end{tikzpicture}
% \end{figure}
%\end{frame}
%
%\begin{frame}{Sentence Classification}
%    Classify input sentences as belonging to the summary or not.
%\end{frame}
%
%
%
%\begin{frame}{Sentence Classification}
%    \begin{itemize}
%        \uncover<2->{\item \textbf{Bayesian Network} classify sentences using 
%                lexical, acoustic, structural, and discourse features. 
%                Surprisingly, summarization seems possible without lexical 
%                features!
%        {\tiny \cite{maskey2005comparing}}}
%
%
%        \uncover<3->{\item \textbf{Hidden Markov Model} predict sequence of
%            sentences to be included in summary, 
%            based on query term and topic signature features
%        {\tiny \cite{conroy2005classy}}}\\
%%    \uncover<3->{
%%        { \centering 
%% \begin{tikzpicture}
%%
%%    \draw (0,2) circle (.26cm);
%%    \draw (1,2) circle (.26cm);
%%    \draw (2,2) circle (.26cm);
%%    \draw (0,1) circle (.26cm);
%%    \draw (1,1) circle (.26cm);
%%    \draw (2,1) circle (.26cm);
%%    \node (X) at (0,2) {$y_1$};
%%    \node (Y) at (1,2) {$y_2$};
%%    \node (Z) at (2,2) {$y_3$};
%%    \node (A) at (0,1) {$s_1$};
%%    \node (B) at (1,1) {$s_2$};
%%    \node (C) at (2,1) {$s_3$};
%%    \path[->] (X) edge (A);
%%    \path[->] (X) edge (Y);
%%    \path[->] (Y) edge (B);
%%    \path[->] (Y) edge (Z);
%%    \path[->] (Z) edge (C);
%% \end{tikzpicture}
%%}}
%\uncover<4->{\item \textbf{Multi Aspect Sentiment} classify sentences as 
%        belonging to an aspect/rating {\tiny \cite{titov2008joint}  }
%        ~\\
%        ~\\
%        \begin{tabular}{|p{9cm}|}
%        \hline
%        \textbf{\alert<5>{Food: 5;} Decor: 5; \alert<6>{Service: 5;} Value: 5}\\
%        \hline
%        \alert<5>{The chicken was great.} On top of that our 
%        \alert<6>{service was 
%        excellent} and the
%        price was right. Can't wait to go back.\\
%        \hline
%    \end{tabular}
%    }
%    \end{itemize}
%\end{frame}
%
%\begin{frame}{Sentence Classification}
%
%Potential flaws:
%\begin{itemize}
%    \item Very little labeled data; limited power of features for prediction
%    \item Classification objective unaware of length constraints; often
%        requires additionall filtering step.
%\end{itemize}
%
%\end{frame}
%\begin{frame}{Extraction by Coverage Optimization}
%
%    \begin{figure}[!h]
%        \centering
% \begin{tikzpicture}
%     \node (C) at (1.5,5) {Classification};
%     \node[rotate=45] (C) at (4.5,-1.7) {Ranking};
%     \node[rotate=-45,text=red,opacity=.7] (C) at (-1.5,-1.7) {Coverage Opt.};
%     \node (C) at (1.5,4) {\includegraphics[scale=.017]{news_icon}};
%     \node (C) at (1.5,3.7) {\tiny \cite{maskey2005comparing}};
%     %\node (C) at (1.5,-1) {\includegraphics[scale=.017]{news_icon}};
%     \node (C) at (0.3,-1.8) {\includegraphics[scale=.022]{news_icon}};
%     \node[text=red,opacity=.7] (C) at (0.3,-2.1) {\tiny \cite{lin2011class}};
%     \node (C) at (-1.8,.3) {\includegraphics[scale=.022]{tweet_icon}};
%     \node[text=red,opacity=.7] (C) at (-1.8,0) {\tiny \cite{liu2011sxsw}};
%     \node (C) at (-.8,-.6) {\includegraphics[scale=.025]{meeting_icon}};
%     \node[text=red,opacity=.7] (C) at (-.8,-.9) {\tiny \cite{gillick2009global}};
%     \node (C) at (-.4,2.2) {\includegraphics[scale=.022]{news_icon}};
%     \node[text=red,opacity=.7] (C) at (-.4,1.9) {\tiny \cite{conroy2005classy}};
%     \node (C) at (1.5,3.3) {\includegraphics[scale=.022]{rating_icon}};
%     \node (C) at (1.5,3.0) {\tiny \cite{titov2008joint}};
%     \node (C) at (4,.4) {\includegraphics[scale=.017]{news_icon}};
%     \node (C) at (4,.1) {\tiny \cite{lin2000automated}};
%     \node (C) at (4,-.2) {\tiny \cite{erkan2004lexrank}};
%     \node (C) at (4,-.5) {\tiny \cite{guo2013updating}};
%     \node (C) at (4,-.8) {\tiny \cite{mccreadie2014incremental}};
%    \draw[opacity=.2,color=red] (0,0) circle (2.5cm);
%    \draw[opacity=.2] (3,0) circle (2.5cm);
%    \draw[opacity=.2] (1.5,3) circle (2.5cm);
% \end{tikzpicture}
% \end{figure}
%\end{frame}
%
%
%
%\begin{frame}{Coverage Optimization} 
%
%Find a subset of input sentences that maximizes the ``coverage'' of the input
%subject to some contraints (typically length).
%
%In this paradigm, coverage must be defined in a way that is positively 
%correlated with the evaluation metric. 
%
%\end{frame}
%
%\begin{frame}{Coverage Optimization}
%    \begin{itemize}
%        \item \textbf{Pivoted QR Decomposition} find an orthonormal basis
%            of the sentence-term matrix, that corresponds to a subset of input
%            sentences. {\tiny \cite{conroy2005classy}}
%\item \textbf{Integer Linear Program} use off-the-shelf solver to find
%    the sentence subset that maximizes the sum of frequency weighted ngrams 
%    {\tiny \cite{gillick2009global,liu2011sxsw}}
%\item \textbf{Submodular Optimization} use greedy algorithm to find 
%    approximate maximum of submodular objective 
%    {\tiny \cite{lin2011class}}
%    \end{itemize}
%\end{frame}
%
%\begin{frame}{Coverage Optimization}
%    \begin{itemize}
%        \item All three approaches are central components of 
%            state-of-the-art extractive newswire summarization systems
%            (cite)
%        \item ILP formulations very flexible with constraint handling;
%            possibly less scalable than QR decomposition or submodular 
%            optimization
%        \item \cite{gillick2009global} match oracle level ROUGE scores;
%            for some domains, extractive summarization may be at the 
%            upper limit of ROUGE scores
%    \end{itemize}
%\end{frame}
%
%
%
%\begin{frame}{Coverage Optimization}
%    \begin{itemize}
%\item \textbf{Integer Linear Program} 
%
%    \begin{align}
%        \max \sum_i w_i c_i&\\
%    \mathrm{s.t.} &\sum_j l_j u_j < L \\
%                  & \sum_j u_j o_{ij} \ge c_i \;\;\; \forall i\\
%        & u_j o_{ij} \le c_i \;\;\; \forall i,j
%    \end{align}
%    where $c_i,u_j, o_{ij}\in \{0,1\}$, $l_j, L \in \mathcal{N}$, $w_i \in \mathcal{R}$\\
%    $c_i$ are concepts (ngrams), $w_i$ is a concept weight (frequency),\\
%    $u_j$ indicates whether sentence $j$ is selected for summary,\\
%    $l_j$ is the length of sentence $j$ and $L$ is the length budget,\\
%    $o_{ij}$ indicates whether concept $i$ occurs in sentence $j$ \\
%
%    \end{itemize}
%\end{frame}
%
%
%\begin{frame}{Heuristics}
%\cite{nenkova2005impact} -- isolate the effects of frequency for extraction
%  
%\begin{itemize}
%\item very competetive ROUGE performance
%\item greedy algorithm for computing summaries 
% \item performance possibly due to reweighting, 
%\begin{itemize}
%\item unreweighted summaries have 
%    much lower ROUGE
%\item unreweighted algo. is approx. of document likelihood objective in \cite{louis2009automatically}, which poorly correlates with human judgements
%\end{itemize}
%\end{itemize}
%
%\end{frame}
%
%\begin{frame}{Heuristics}
%\cite{nenkova2005impact} -- examine weighting pyramid annotations (potentially remove need for model 
%    summary annotation)
%\begin{itemize}
%\item strong but imperfect correlation
%\item frequency does not fully explain content selection
%\begin{figure}
%\centering
%\begin{tabular}{l| l| l| l}
%         & Top 5              & Top 8              & Top 12  \\
%\hline
%human    & \alert<2>{94.66\%} & \alert<3>{91.25\%} & \alert<4>{85.25\%} \\
%\hline
%machine  & 84.00\%            & 77.87\%            & 66.08\% \\
%\hline
%SumBasic & \alert<2>{96.00\%} & \alert<3>{95.00\%} & \alert<4>{90.83\%} \\
%\end{tabular}
%
%\end{figure}
%\item SumBasic consistently \alert<2->{over-estimates} frequency importance vs human
%\end{itemize}
%\end{frame}
%
%\begin{frame}{Heuristics}
%\cite{lin2000automated} -- automatic method for finding key phrases/terms 
%    using likelihood ratios
%\begin{itemize}
%\item $\mathcal{R} \triangleq$ relevant docs; 
%    $\tilde{\mathcal{R}} \triangleq$ irrelevant docs;
%    $t \triangleq $ ngram 
%\item Hypothesis 1 ($H_1$): $P(\mathcal{R}|t) = P(\tilde{\mathcal{R}}|t)$
%\item Hypothesis 2 ($H_2$): $P(\mathcal{R}|t) \gg P(\tilde{\mathcal{R}}|t)$
%\item large $L(H_2) \rightarrow $ large  $ -2\log\frac{L(H_1)}{L(H_2)} $
%\end{itemize}
%\end{frame}
%
%
%\begin{frame}{Heuristics}
%\cite{lin2000automated} -- automatic method for finding key phrases/terms 
%    using likelihood ratios
%\begin{itemize}
%\item ranking sentences by topic signatures outperforms tfidf and lead 
%    baselines
%\item limited evaluation (only 4 topics)
%\item however, \cite{louis2009automatically} find moderate positive 
%    correlation between topic signature coverage and human responsiveness 
%    scores
%\end{itemize}
%\end{frame}
%
%\begin{frame}{Random Walks on Sentences}
%\cite{erkan2004lexrank} -- Extract sentences via graph-based notion of 
%    centrality
%\begin{itemize}
%\item sentences are \textit{nodes} in a \textit{graph}, 
%\item \textit{edges} are weighted by sentence similarity
%\item PageRank finds an eigenvector of the adjacency matrix
%\item eigenvector elements are interpretable as a ranking of sentence centrality
%\end{itemize}
%\end{frame}
%
%\begin{frame}{Random Walks on Sentences}
%\cite{erkan2004lexrank} -- Extract sentences via graph-based notion of 
%    centrality
%\begin{itemize}
%\item language agnostic
%\item generally benefits from reranking to handle redundancy
%\end{itemize}
%\end{frame}
%
%\begin{frame}{Query Focused Summarization}
%\cite{conroy2005classy} -- use query term and topic signature matches as 
%    features in an hidden Markov model (HMM).
%\begin{itemize}
%\item selection with HMM, requires extractive gold summaries
%\item reranking step performed with pivoted QR decomposition
%\item Named Entity (NE) tagging not helpful, despite HACKY exploitation of 
%    ROUGE-1
%\item High pyramid scores, despite low ROUGE-1 performance
%\end{itemize}
%\end{frame}
%
%
%\begin{frame}{Streaming Summarization}
%\cite{guo2013updating} -- define streaming summarization task, exploratory
%    system design and evaluation
%\begin{itemize}
%\item explore sentence selection from first 10 sentences or headline only
%\item effects of stationary/non-stationary features
%\item non-stationary features introduce importance of buffering policy
%\item feature combination for first 10 sentences best overall
%\end{itemize}
%\end{frame}
%
%\begin{frame}{Streaming Summarization}
%\cite{mccreadie2014incremental} -- Streaming summarization systems are noisy
%    during periods of downtime; learn to predict rank cutoff to reduce noise
%\begin{itemize}
%\item this task is hard -- very large gap between oracle and current top 
%    performance
%\item improved recall measure but precision? Counter-intuitive result? 
%\end{itemize}
%\end{frame}
%
%\begin{frame}{Summarization in other Domains}
%\cite{maskey2005comparing} -- Broadcast news speech summarization
%\begin{itemize}
%\item Prosodic/Acoustic features complement lexical features
%\item Suggests speech summarization without transcription is possible
%\end{itemize}
%\end{frame}
%
%\begin{frame}{Summarization in other Domains}
%\cite{titov2008joint} -- Review/Opinion summarization; Correlate LDA-style 
%    topics with ratings values
%\begin{itemize}
%\item model allows for extraction of sentences for different aspects of review
%\item very competitive with a supervised extractive model 
%\end{itemize}
% 
%\end{frame}
%
%
%\begin{frame}{Summarization in other Domains}
%\cite{gillick2009global} -- Meeting summarization; use ILP solver to maximize
%    ngram coverage subject to length and speaker constraints
%\begin{itemize}
%\item were able to obtain oracle ROUGE scores
%\item remaining improvement should be found in improving coherence and other
%    linguistic qualities 
%\end{itemize}
%\end{frame}
%
%\begin{frame}{Summarization in other Domains}
%\cite{liu2011sxsw} -- Twitter summarization; use ILP solver to maximize
%    ngram coverage subject to length and speaker constraints
%\begin{itemize}
%\item compare multiple input sources: tweets/normalized tweets or linked web
% text, or combination 
%\item only tiny variation in ROUGE depending on input
%\item Web Text inputs yield higher grammaticality scores
%\item Tweet inputs yield higher clarity and focus scores
%\end{itemize}
%\end{frame}
%
%
%\section{Abstraction}
%
%\begin{frame}{Abstraction Paradigms}
%    \begin{itemize}
%        \item \textbf{Sentence Fusion} combine several similar sentences into
%            one sentence
%        \item \textbf{Compression} selectively remove words/phrases that 
%            are less important 
%    \end{itemize}
%\end{frame}
%
%\begin{frame}{Fusion in MDS} 
%    \begin{enumerate}
%        \item Content Aggregation, (theme selection in \cite{barzilay2005sentence}), i.e. cluster input sentences.
%        \item Order clusters.
%        \item Fuse clusters.
%    \end{enumerate}
%\end{frame}
%
%\begin{frame}{Sentence Fusion}
%    \cite{barzilay2005sentence} -- Combine dependency graphs of similar 
%    sentences; linearize with a language model
%    \begin{itemize}
%    %\item performs text-to-text generation without semantic representation
%    %\item fusion lattice structure contains multiple verbalizations
%    \item output has low content coverage compared to humans;
%        but better than baseline
%    \item linearization is the source of grammatical errors
%    \end{itemize}
%
%%\end{frame}
%
%%\begin{frame}{Sentence Fusion}
%    \cite{filippova2008sentence} -- sentence fusion by finding compressed 
%    dependency graph using ILP formulation
%    \begin{itemize}
%    \item deletion conditioned on informativeness and dependency labeled 
%        scores derived from corpus
%    \item better human readability scores than \cite{barzilay2005sentence}
%    \end{itemize}
%
%    Both system evaluated on small, non-standard datasets. \\
%    Hard to say how comparable to competitive extractive system.
%
%\end{frame}
%
%
%
%\begin{frame}{Compression with Extraction: Heuristic Pruning} 
%    \begin{enumerate}
%        \item Generate many compressed sentences.
%        \item Rank/Order sentences.
%    \end{enumerate}
%\cite{zajic2006sentence} -- Applied single-sentence compression to 
%    over-generate possible summary sentences; greedily select sentences
%    for summary inclusion
%    \begin{itemize}
%    \item Manually tuned weights -- data-driven approach likely to give better
%        results
%    \item moderate to poor ROUGE results
%    \item poor linguistic quality judgements
%    \end{itemize}
%\end{frame}
%
%\begin{frame}{Compression in with Extraction: Beam Search} 
%    \begin{enumerate}
%        \item Rank/Order sentences.
%        \item Compress sentences.
%    \end{enumerate}
%    \cite{wang2013sentence} -- rank and compress approach, adding compressed
%    sentences to summary until length constraint exhausted
%    \begin{itemize}
%        \item pipeline approach, ranking and generation trained separately 
%        \item ranking performed on uncompressed sentences
%        \item compression scoring uses beam search to handle constraints of 
%            a simple classification model -- possibly a better solution to this?
%    \end{itemize}
%\end{frame}
%
%
%
%
%\begin{frame}{Compression with Extraction: ILP}
%    \cite{martins2009summarization} -- Sentence extraction and compression
%    task is encoded as an ILP; solve both problems simultaneously. 
%    \begin{itemize}
%    %\item flexible model for encoding various constraints
%    \item extraction parameters trained separately from compression parameters
%    \item only see small increase in ROUGE-2 scores against extractive baselines
%    \end{itemize}
%
%    \cite{berg2011jointly} -- learning and predicting for joint compression and
%    extraction, using ILP formulation
%    \begin{itemize}
%    %\item created bigram recall maximizing overlength extracts
%    \item used mechanical turk to collect compressions of overlength extracts
%    \item approximate joint objective: 
%        \begin{itemize}
%            \item run overlength extractive solver
%            \item run compressive solver over fixed extractions
%        \end{itemize}
%    \end{itemize}
%\end{frame}
%
%\begin{frame}{Other Abstractive Frameworks}
%    \begin{itemize}
%\item \textbf{Quasi-Synchronous Grammar} Headline/Caption generation using ILP 
%    solver and quasi-synchronous grammar alignment constraints 
%    {\tiny \cite{woodsend2010generation}}
%\item \textbf{OpenIE} Mine template patterns for sentence summarization
%    {\tiny \cite{pighin2014modelling}}
%\item \textbf{Semantic Summarization} Merges AMR graphs of sentence inputs 
%    into a single graph object; after graph compression applied, generation 
%    can be performed (in theory)
%    {\tiny \cite{liu2015toward}}
%\item \textbf{Attentional Neural Networks}
%    adapt neural machine translation to headline generation
%    {\tiny \cite{rush2015neural}}
%    \end{itemize}
%\end{frame}
%
%\begin{frame}{Summarization with Grammar}
%    \cite{woodsend2010generation} -- Headline/Caption generation using ILP 
%    solver and quasi-synchronous grammar alignment constraints
%    \begin{itemize}
%    \item Similar to ILP's for compression/extraction, but more expressive:
%       \begin{itemize}
%           \item deletion (compression)
%           \item rewriting (paraphrase)
%           \item reordering 
%       \end{itemize}
%   \item high grammaticality and importance scores (on par with human 
%       reference)
%    \end{itemize}
%\end{frame}
%
%\begin{frame}{Summarization with Open-IE patterns}
%
%    \cite{pighin2014modelling} -- Mine template patterns for sentence 
%        summarization
%        \begin{itemize}
%            \item propose a memory-based look up data structure for mapping a 
%                sentence to IE-pattern sub-graphs it contains
%            \item only compare against their own template based methods;
%                unclear how these compare other approaches or human reference
%        \end{itemize}
%
%\end{frame}
%
%\begin{frame}{Summarization with Semantics}
%    \cite{liu2015toward} -- Merges AMR graphs of sentence inputs into a single
%    graph object; after graph compression applied, generation can be performed
%    (in theory)
%    \begin{itemize}
%    \item uses ILP solver to find graph compression that matches summary graph
%    \item generation from resultant graph is an open problem
%    \item simple unigram generation model used as proof of concept; 
%        (much ceiling between achieved results and oracle) 
%    \end{itemize}
%\end{frame}
%
%\begin{frame}{Headline Generation}
%    \cite{rush2015neural} -- adapt neural machine translation to headline 
%        generation
%    \begin{itemize}
%        \item alignments at scale are very effective (abstractive attention 
%            model and trad. MT system MOSES are very competitive)
%        \item neural model requires additional beam search decoding features
%            to surpass MOSES
%    \end{itemize}
%\end{frame}
%
%
%
%
%
%
%\begin{frame}{Abstractive Summarization - Approaches}
%    \begin{figure}
%          \small
%        \rowcolors{2}{gray!5}{white}
%    \begin{tabular}{l | l | l |}
%        \hline
%        &  \multicolumn{2}{c|}{Approach } \\
%    & Fuse & Compress \\
%        \hline
%        \cite{barzilay2005sentence} & X &  \\
%        \hline
%        \cite{zajic2006sentence}    & & X \\
%        \hline
%        \cite{filippova2008sentence} & X & \\
%        \hline
%        \cite{martins2009summarization} & & X \\
%        \hline
%        \cite{woodsend2010generation}  &  & X \\
%        \hline
%        \cite{berg2011jointly} & & X\\
%        \hline
%        \cite{wang2013sentence} & & X \\
%        \hline
%        \cite{pighin2014modelling} & &  \\
%        \hline
%        \cite{liu2015toward} & X & X \\
%        \hline
%        \cite{rush2015neural} & & X \\
%        \hline
%    \end{tabular}
%\end{figure}
%
%
%\end{frame}
%
%
%\begin{frame}{Abstractive Summarization - Approaches}
%    \begin{figure}
%          \small
%        \rowcolors{2}{gray!5}{white}
%    \begin{tabular}{l | l | l || l | l |}
%        \hline
%        &  \multicolumn{2}{c|}{Approach } &  \multicolumn{2}{c|}{Inference}  \\
%        & Fuse & Compress & ILP & Search\\
%        \hline
%        \cite{barzilay2005sentence} & X & &  & X \\
%        \hline
%        \cite{zajic2006sentence}    & & X & & X\\
%        \hline
%        \cite{filippova2008sentence} & X & & X & X\\
%        \hline
%        \cite{martins2009summarization} & & X & X&\\
%        \hline
%        \cite{woodsend2010generation}  &  & X  & X&\\
%        \hline
%        \cite{berg2011jointly} & & X & X &\\
%        \hline
%        \cite{wang2013sentence} & & X & &  X \\
%        \hline
%        \cite{pighin2014modelling} & &  & & X \\
%        \hline
%        \cite{liu2015toward} & X & X & X & \\
%        \hline
%        \cite{rush2015neural} & & X &  & X \\
%        \hline
%    \end{tabular}
%\end{figure}
%
%
%\end{frame}
%
%\begin{frame}{Fusion}
% \begin{tikzpicture}[
%    dep/.style={shape=circle,draw=black, align=center, scale=.4}]
%\node[dep] (A) at (0,0) {confirm};
%\node[dep] (B) at (-1,-1) {IDF \\ spokeswoman};
%\node[dep] (C) at (0,-1) {this};
%\node[dep] (D) at (1,-1) {but};
%\node[dep] (E) at (1,-2) {said};
%\node[dep] (F) at (1,-3) {fire};
%\node[dep] (G) at (1,-4) {antitank \\ missile};
%\node[dep] (H) at (-.5,-4) {Palestinian};
%\node[dep] (I) at (2,-4) {at};
%\node[dep] (J) at (2,-5) {bulldozer};
%\path [->] (A) edge (B); 
%\path [->] (A) edge (C); 
%\path [->] (A) edge (D); 
%\path [->] (D) edge (E); 
%\path [->] (E) edge (F); 
%\path [->] (F) edge (G); 
%\path [->] (F) edge (H); 
%\path [->] (F) edge (I); 
%\path [->] (I) edge (J); 
%
%
%\node[dep] (1) at (5,0) {erupt};
%\node[dep] (2) at (4.5,-1) {clash};
%\node[dep] (3) at (5.5,-1) {when};
%\node[dep] (4) at (5.5,-2) {fire};
%\node[dep] (5) at (4.0,-3.5) {Palestinian \\ militant};
%\node[dep] (6) at (5.5,-3.5) {machine gun \\ and antitank \\ missile};
%\node[dep] (7) at (6.5,-3.5) {at};
%\node[dep] (8) at (6.5,-4.5) {that};
%\node[dep] (9) at (6.5,-5.5) {build};
%\node[dep] (10) at (5.5,-6.5) {embarkment};
%\node[dep] (11) at (6.5,-6.5) {in};
%\node[dep] (12) at (6.5,-7.5) {area};
%\node[dep] (13) at (7.5,-6.5) {protect};
%\node[dep] (14) at (7.5,-7.5) {better};
%\node[dep] (15) at (8.5,-7.5) {Israeli \\ force};
%
%\path [->] (1) edge (2); 
%\path [->] (1) edge (3); 
%\path [->] (3) edge (4); 
%\path [->] (4) edge (5); 
%\path [->] (4) edge (6); 
%\path [->] (4) edge (7); 
%\path [->] (7) edge (8); 
%\path [->] (8) edge (9); 
%\path [->] (9) edge (10); 
%\path [->] (9) edge (11); 
%\path [->] (9) edge (13); 
%\path [->] (11) edge (12); 
%\path [->] (13) edge (14); 
%\path [->] (13) edge (15); 
%
%
%\end{tikzpicture}  
%\end{frame}
%
%\begin{frame}{Abstractive Summarization}
%
%    \begin{tabular}{l l l}
%        &   Single Sentence Generation & Multiple Sentence Generation\\
%\cite{barzilay2005sentence}  \\
%        \cite{zajic2006sentence} \\
%        \cite{filippova2008sentence} \\
%        \cite{martins2009summarization} \\
%        \cite{woodsend2010generation} \\
%        \cite{berg2011jointly} \\
%        \cite{wang2013sentence} \\
%        \cite{pighin2014modelling} \\
%        \cite{liu2015toward} \\
%        \cite{rush2015neural} \\
%
%
%
%    \end{tabular}
%
%
%\end{frame}
%
%\begin{frame}{Factoid and Pyramid}
%
%\begin{tcolorbox}[title=Factoids] 
%\begin{itemize}
%   \item \textbf{Factoid} $\triangleq$ atomic information units
%    \item definition includes subsumption, equivalence rules  
%    \item strong agreement in annotation, moderate aggrement in factoid definition
%\end{itemize}
%\end{tcolorbox}
%
%\begin{tcolorbox}[title=Pyramid] 
%\begin{itemize}
%    \item \textbf{Summary Content Units (SCUs)} $\triangleq$ sub-sentential clauses,
%     whose meaning is repeated across summaries 
%    \item guidelines are purposely open-ended on what consitutes an SCU
%\end{itemize}
%\end{tcolorbox}
%\end{frame}
%
%\begin{frame}{Factoid and Pyramid}
%\begin{tcolorbox}[title=Factoid \& Pyramid] 
%\begin{itemize}    
%    \item weights assigned to each factoid/SCU based on the frequency of 
%        reference summaries it occurs in
%    \item summary score is proportional to the sum of the weights of the 
%        factoids/SCUs contained within
%\end{itemize}
%\end{tcolorbox}
%\end{frame}
%
%
%
%
%\begin{frame}{Factoid and Pyramid}
%
%Disagreement in annotator stability:
%
%\begin{itemize}
%\item requires 20-30 reference summaries \cite{teufel2004evaluating}
%\item requires at least 5 reference summaries \cite{nenkova2007pyramid}
%\item requires at least 2-3 reference summaries for correlation with 
%    responsiveness \cite{owczarzak2009evaluation}
%\end{itemize}
%
%
%\end{frame}
%
%
%
%\begin{frame}{Summary Evaluation (support slide)}
%\textbf{Linguistic Quality Assessment}, humans judged summaries on 5 qualities:
%\begin{enumerate}
%\item Grammaticality
%\item Non-redundancy
%\item Referential clarity
%\item Focus
%\item Structure and Coherence
%\end{enumerate}
%
%Assessors determined ratings on a 5 point scale.
%
%\end{frame}
%
%\begin{frame}{Summary Evaluation (support slide)}
%\textbf{Coverage Assessment}, using a single reference summary, how much of 
%    the meaning in the reference is covered by the system.
%
%Assessors chose on 6 point scale:\\
% (~0\%~~~~20\%~~~~40\%~~~~60\%~~~~80\%~~~~100\%~)
%
%\end{frame}
%
%
%
%
%\begin{frame}{Summary Evaluation (support slide)}
%\textbf{Responsiveness Assessment}, humans judged how well a system summary answers the
%    information need, given the topic query/user profile.
%
%Assessors determined ratings on a 5 point scale. Score is \textbf{relative} 
%    within each topic. 
%
%``The linguistic quality of
%the summary should play a role in your assessment only insofar as it
%interferes with the expression of information and reduces the amount
%of information that is conveyed.'' -- DUC Guidelines
%
%\end{frame}
%
%\begin{frame}{Summary Evaluation: DUC 2005/2006/2007}
%
%    In addition to the NIST evaluation, \\~\\
%
%
%    USC/ISI:\\
%    ~~~~~ROUGE, Basic Elements\\
%~\\
%
%    Columbia U.:\\
%    ~~~~~Pyramid
%
%\end{frame}
%
%\begin{frame}{Summary Evaluation}
%    \begin{figure}
%          \small
%        \rowcolors{2}{gray!5}{white}
%\begin{tabular}{c | c c || c c c c|}
%        &  \multicolumn{2}{c||}{Automatic } 
%        &  \multicolumn{4}{c|}{Manual } \\
%Year & \alert<2>{ROUGE} & \alert<2>{BE} & \alert<2>{Pyramid} & Ling. Quality & Respons. & \alert<2>{Coverage}\\
% \hline 
%'01 & & & &X & & X\\
%'02 & & & &X & & X\\
%'03 & & & &X & X & X\\
%'04 & X & & &X & X & X\\
%'05 & X & X &X &X &X& \\
%'06 & X & X &X &X &X& \\
%'07 & X & X &X &X &X& \\
%'08 &  &  &X & &X& \\
%'09 &  &  &X &X &X& \\
%'10 &  &  &X &X &X& \\
%'11 &  &  &X &X &X& \\
%\hline
%\end{tabular}
%\end{figure}
%\uncover<2>{\alert<2>{*Requires a reference summary}}
%\end{frame}
%
%
%\begin{frame}{Desiderata of Evaluation Metric}
%
%\begin{itemize}
%\item System rankings should be relatively stable given
%\begin{itemize}
%\item different human annotators \uncover<2->{(How much human variation?)}
%\item different reference summaries \uncover<3->{(How many references?)}
%\item given different topics/document sets \uncover<4->{(How many topics?)}
%\end{itemize}
%~\\
%\item Automatically computable
%\begin{itemize}
%\item at least given reference summaries
%\end{itemize}
%\end{itemize}
%\end{frame}
%
%\begin{frame}{Content Selection Metrics}
%Requires Reference
%\begin{itemize}
%\item ROUGE -- \cite{lin2004rouge}
%\item Factoids -- \cite{teufel2004evaluating}
%\item Pyramid -- \cite{nenkova2007pyramid}
%\begin{itemize}
%\item LDA -- \cite{hennig2010learning}
%\item PEAK -- \cite{yang2016peak}
%\end{itemize}
%\end{itemize}
%Reference Free Coverage Metrics
%\begin{itemize}
%\item Jensen-Shannon Divergence -- \cite{louis2009automatically}
%\end{itemize} 
%\end{frame}
%
%
%
%
%
%
%
%\begin{frame}{Example Factoid Analysis}
%
%Summary Sentence: \textit{The police have arrested a white Dutch man.}
%
%\begin{itemize}
%\item A suspect was arrested
%\item The police did the arresting
%\item The suspect is white
%\item The suspect is Dutch
%\item The suspect is male
%\end{itemize}
%
%\end{frame}
%
%
%\begin{frame}{Example SCU}
%SCU annotation: Lopez left GM for VW
%\begin{itemize}
%\item \small{
%    The industrial espionage case involving GM and VW began with 
%    \underline{the hiring of Jose Ignacio Lopez, an employee of GM} 
%    subsidiary Adam Opel,  \underline{by VW} as a production director.}
%\item \small{
%    However, \underline{he left GM for VW} under circumstances, which along
%    with ensuing events, were described by a German judge as ``potentially the
%    biggest ever case of industrial espionage.''}
%\item \small{
%    \underline{He left GM for VW} in March 1993.}
%\item \small{
%    The issue stems from the alleged \underline{recruitment of GM's} eccentric
%    and visionary Basque-born procurement chief \underline{Jose Ignacio Lopez}
%    de Arriortura and seven of Lopez's business colleagues.}
%%\item Agnacio Lopez De Arriortua, left his job ... at General Motor's Opel 
%%        ... to become Volkswagen's ... director
%\item \small{
%    In March 1993, \underline{Lopez} and seven other \underline{GM} executives 
%    \underline{moved to VW} overnight.}
%\end{itemize}
%\end{frame}
%
%
%
%
%
%
%\begin{frame}{Reliability of Metrics}
%
%
%
% \begin{tikzpicture}[
%    dep/.style={shape=circle,draw=black, align=center}]
%\node[] (A) at (0,0) {};
%\node[] (B) at (6,0) {$\ldots$};
%\node[] (C) at (0,6) {};
%\path [-] (A) edge (5.5,0); 
%\path [->] (6.5,0) edge (8.5,0); 
%\path [->] (A) edge (C); 
%\path [-] (1,-.25) edge (1,.25); 
%\path [-] (2,-.25) edge (2,.25); 
%\path [-] (3,-.25) edge (3,.25); 
%\path [-] (4,-.25) edge (4,.25); 
%\path [-] (7,-.25) edge (7,.25); 
%\path [-] (8,-.25) edge (8,.25); 
%\node[] (ZZ) at (7,-.5) {20};
%\node[] (ZZ) at (8,-.5) {30};
%
%\path [-] (-.25,1) edge (.25,1); 
%\path [-] (-.25,2) edge (.25,2); 
%\path [-] (-.25,3) edge (.25,3); 
%\path [-] (-.25,4) edge (.25,4); 
%\node (XXX) at (1,-.5) {1};
%\node (XXX) at (2,-.5) {2};
%\node (XXX) at (3,-.5) {3};
%\node (XXX) at (4,-.5) {4};
%\node (XXX) at (-.5,1) {12};
%\node (XXX) at (-.5,2) {24};
%\node (XXX) at (-.5,3) {36};
%\node (XXX) at (-.5,4) {48};
%
%%\path [->] (A) edge (D); 
%%\path [->] (D) edge (E); 
%
%
%\node[] (C) at (2,3) {Pyramid};
%\node[] (C) at (3,3.5) {ROUGE};
%\node[] (C) at (7.5,2) {Factoid};
%\path[-|] (7.5,2.5) edge (7.5,5.5);
%\path[-|] (7.5,1.5) edge (7.5,0.2);
%\node[] (C) at (3,-1) {num. of references};
%\node[rotate=90] (C) at (-1,3) {num. of topics};
%
%\end{tikzpicture}
%
%
%\end{frame}
%
%
%
%\begin{frame}{Evaluating Update Summaries}
%\cite{conroy2011nouveau} -- there is a gap between automatic metrics of human 
% update summaries and machine generated summaries
%\begin{figure}
%\centering
%\includegraphics[width=\linewidth,height=.7\textheight,keepaspectratio]{metric_gap_rouge.jpg}
%\end{figure}
%\end{frame}
%
%\begin{frame}{Evaluating Update Summaries}
%\cite{conroy2011nouveau} -- there is a gap between automatic metrics of human 
% update summaries and machine generated summaries
%\begin{itemize}
%\item $NouveauRouge = \alpha_{AB} Rouge(A,B) + \alpha_{BB} Rouge(B,B) + \alpha_0$
%\item $Rouge(A,B) \triangleq$ ROUGE score of system update summary to 
%         model original summary
%\item $Rouge(B,B) \triangleq$ ROUGE score of system update summary to 
%         model update summary
%\item in fitted model: 
%\begin{itemize}
%\item $\alpha_{A,B} < 0$ 
%\item $\alpha_{B,B} > 0$ 
%\end{itemize}
%\end{itemize}
%
%\end{frame}
%
%
%
%
%%\begin{frame}{Responsiveness 2003} 
%%Query focused summarization $\rightarrow$ \textbf{intrinsic} evaluation: 
%%Responsiveness
%%
%%\small{
%%You have been given a question (topic), the relevant sentences from a document set, and a number of short summaries of those sentences - designed to answer the question. Some of the summaries may be more responsive (in form and content) to the question than others. Your task is to help us understand how relatively well each summary responds to the question.
%%Read the question and all the associated short summaries. Consult the relevant sentences in the document set as needed. Then grade each summary according to how responsive it is to the question: 0 (worst, unresponsive), 1, 2, 3, or 4 (best, fully responsive). 
%%}
%%\end{frame}
%%
%%
%%
%%\begin{frame}{Evaluating Summarization}
%%\cite{lin2004rouge} -- ROUGE is the most prevalent automatic measurement tool
%% in use for summarization task.
%%\begin{itemize}
%%\item recall focused, measures average overlap of system/human summary ngrams
%%\item many shades: ngram sizes 1 - 9, skip-grams, longest common subsequence 
%%\item Strong correlation with single doc human coverage scores
%%\item Reasonable correlation with multi-doc converage scores 
%%    (largest sample size only 59)
%%\end{itemize}
%%\end{frame}
%%
%%\begin{frame}{Evaluating Summarization}
%%Criticisms of ROUGE:
%%\begin{itemize}
%%\item insensitive to lexical variation
%%\item uniformly rewards sequence matches, e.g.``of the'' probably not indicative of anything
%%\item insensitive to linguistic quality, e.g. coherence or disource
%%\item correlation for MDS is only strong at the 200 and 400 word length
%%\end{itemize}
%%\end{frame}
%%
%%\begin{frame}{Evaluating Summarization}
%%
%%\cite{hovy2006automated} -- Basic Elements (BE) uses phrase head/modifier words
%% as unit of analysis
%%
%%\begin{itemize}
%%\item High correlation with responsivenss scores and ROUGE
%%\item Unclear if BE tells us anything ROUGE doesn't
%%\item more computationally expensive -- requires parser
%%\item \cite{owczarzak2009evaluation} found BE to be more unstable under different number of model summaries
%%\end{itemize}
%%\end{frame}
%%
%%\begin{frame}{Evaluating Summarization}
%%\cite{teufel2004evaluating} develop annotation guidelines for factoid 
%%definition and annotation, summaries ranked by \# of factoids they contain
%%\begin{itemize}
%%\item high agreement on annotation
%%\item moderate agreement on definition
%%\item takes 20-30 reference summaries for ranking to stabilize!
%%\item poor correlation to DUC information overlap?!
%%\end{itemize} 
%%\end{frame}
%%
%%\begin{frame}{Evaluating Summarization}
%%\cite{nenkova2007pyramid} -- Pyramid: annotation scheme for extracting 
%%    summary content units (SCUs) from model summaries \\
%%    system summaries score by importance weighted sum of SCUs they contain
%%\begin{itemize}
%%\item score differences stabilize with 4-5 model summaries (compared to 
%%    20-30 in \cite{teufel2004evaluating})
%%\item moderate interannotator aggreement on peer annotation
%%\item strong interannotator score correlation 
%%\end{itemize} 
%%
%%\end{frame}
%%
%%\begin{frame}{Explaining Variation in Evaluation Metrics}
%%
%%\cite{nenkova2005automatic} -- ANOVA on official DUC coverage scores
%%\begin{itemize}
%%\item controlled for system and input (and length in MDS)
%%\item (generic MDS) differences between systems and baseline are not 
%%    significant for word lengths 50 and 100  
%%\item  (generic SDS) no system outperforms baseline; 8/10 humans outperform 
%%    baseline 
%%\end{itemize}
%%
%%\end{frame}
%%
%%\begin{frame}{Explaining Variation in Evaluation Metrics}
%%\cite{owczarzak2009evaluation} -- examines stability of system ranking with
%%    automatic evaluation against TAC Responsiveness Scores  
%%\begin{itemize}
%%\item examined Pyramid, ROUGE-2, ROUGE-SU4, and Basic Elements
%%\item correlation to manual metrics stabilizes after 2 models
%%\item Contrary to \cite{teufel2004evaluating}, argues for more system inputs 
%%    with fewer human models 
%%\end{itemize}
%%\end{frame}
%
%\begin{frame}{Summary Evaluation}
%
%    \begin{itemize}
%        \item evaluation w/o model seems possible
%        \item potential issues with evaluation of update/query summarization
%        \item eventual breakdown of content selection metrics?
%    \end{itemize}
%
%\end{frame}

\bibliographystyle{apalike}
\bibliography{references}


\end{document}

